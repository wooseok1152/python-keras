{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_attached_with_index : {'The': 1, 'cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat.': 6, 'dog': 7, 'ate': 8, 'my': 9, 'homework.': 10} \n",
      "\n",
      "number 1 text sample's index and token : [(0, 'The'), (1, 'cat'), (2, 'sat'), (3, 'on'), (4, 'the'), (5, 'mat.')]\n",
      "number 2 text sample's index and token : [(0, 'The'), (1, 'dog'), (2, 'ate'), (3, 'my'), (4, 'homework.')]\n",
      "\n",
      " tokenized result : \n",
      " [[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 단어 수준의 원-핫 인코딩하기(keras의 tokenizer를 사용하지 않고)\n",
    "text_samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
    "token_attached_with_index = {}\n",
    "\n",
    "# 해당 중첩 for문을 통해서 토큰화가 진행되고, 각 토큰에(단어에) 고유 index가 주어진다.\n",
    "for sample in text_samples:\n",
    "    \n",
    "    for word in sample.split():\n",
    "        \n",
    "        if word not in token_attached_with_index:\n",
    "            \n",
    "            token_attached_with_index[word] = len(token_attached_with_index) + 1\n",
    "\n",
    "print(\"token_attached_with_index :\", token_attached_with_index, \"\\n\")\n",
    "\n",
    "all_tokens_count = len(token_attached_with_index.keys())\n",
    "\n",
    "result = np.zeros(shape = (len(text_samples), all_tokens_count, max(token_attached_with_index.values())+1))\n",
    "\n",
    "sample_number = 1\n",
    "for i, sample in enumerate(text_samples):\n",
    "    \n",
    "    for j, word in list(enumerate(sample.split()))[:all_tokens_count]:\n",
    "        \n",
    "        index = token_attached_with_index.get(word)\n",
    "        result[i, j, index] = 1.\n",
    "    print(\"number\", sample_number, \"text sample's index and token :\", list(enumerate(sample.split()))[:all_tokens_count])\n",
    "    sample_number = sample_number + 1\n",
    "    \n",
    "print(\"\\n\", \"tokenized result :\", \"\\n\", result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized 결과 : {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9} \n",
      "\n",
      "text samples expressed with token's index : [[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]] \n",
      "\n",
      "text samples expressed with matrix :\n",
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]] \n",
      "\n",
      "9개의 고유한 토큰을 찾았음\n"
     ]
    }
   ],
   "source": [
    "# keras의 Tokenizer 객체를 이용하여 토큰화 실시\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "text_samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
    "\n",
    "# tokenizer 객체 생성\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "\n",
    "# tokenizer를 주어진 text samples에 적용\n",
    "tokenizer.fit_on_texts(text_samples)\n",
    "\n",
    "# token과 그에 부여한 index 확인\n",
    "print(\"tokenized 결과 :\", tokenizer.word_index, \"\\n\")\n",
    "\n",
    "# 앞선 토큰화를 토대로, 정수로 인코딩된 텍스트 데이터를 반환\n",
    "sequences = tokenizer.texts_to_sequences(text_samples)\n",
    "print(\"text samples expressed with token's index :\", sequences, \"\\n\")\n",
    "\n",
    "# 앞선 토큰화를 토대로, 원 핫 인코딩된 결과를 반환\n",
    "print(\"text samples expressed with matrix :\")\n",
    "one_hot_result = tokenizer.texts_to_matrix(text_samples, mode=\"binary\")\n",
    "print(one_hot_result, \"\\n\")\n",
    "\n",
    "print(str(len(tokenizer.word_index.keys())) + \"개의 고유한 토큰을 찾았음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [1 6 7]]\n"
     ]
    }
   ],
   "source": [
    "#  embadding을 하기 위해 정수로 인코딩 된 텍스트 데이터들의 길이를 동일하게 맞추기\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "padding_result = preprocessing.sequence.pad_sequences(sequences, maxlen=3, truncating=\"post\")\n",
    "print(padding_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
